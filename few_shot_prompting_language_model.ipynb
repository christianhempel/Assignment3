{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1 implement few shot prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wheus\\AppData\\Roaming\\Python\\Python38\\site-packages\\huggingface_hub\\file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load the GPT-2 model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model.eval()\n",
    "\n",
    "# TODO: Prepare the few-shot prompt with examples\n",
    "few_shot_prompt = \"\"\"\n",
    "Here is a classification of the review of some movies and their sentiment which is either positive or negative:\n",
    "\n",
    "Review: The movie was fantastic, with stunning visuals and a great story.\n",
    "Sentiment: Positive\n",
    "Review: The acting was top-notch.\n",
    "Sentiment: Positive\n",
    "Review: I found the film boring and overly long. Not worth watching.\n",
    "Sentiment: Negative\n",
    "Review: The characters were shallow, and the story was predictable.\n",
    "Sentiment: Negative\n",
    "\"\"\"\n",
    "\n",
    "# TODO: Define a function to get GPT-2 prediction for a new sentence\n",
    "def get_gpt2_prediction(sentence):\n",
    "    prompt = few_shot_prompt + f\"Review: {sentence}\\nSentiment:\"\n",
    "    \n",
    "    # Encode with padding\n",
    "    inputs = tokenizer(prompt, return_tensors='pt', padding=True)\n",
    "    \n",
    "    # Pass attention_mask and pad_token_id to generate()\n",
    "    outputs = model.generate(\n",
    "        inputs['input_ids'],\n",
    "        attention_mask=inputs['attention_mask'],\n",
    "        max_length=inputs['input_ids'].shape[1] + 10,\n",
    "        do_sample=False,\n",
    "        num_beams=5,\n",
    "        early_stopping=True,\n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "    \n",
    "    # Convert generated text to lowercase for more robust matching\n",
    "    generated_text = tokenizer.decode(outputs[0]).lower()\n",
    "    #print(\"\\n\\n\", generated_text, \"\\n\\n\")\n",
    "    generated_text = \"review:\" + generated_text.split(\"review:\")[-1]\n",
    "    #print(generated_text)\n",
    "\n",
    "    \n",
    "    # Post-process to extract the sentiment prediction\n",
    "    if \"positive\" in generated_text:\n",
    "        print('positive')\n",
    "        return \"Positive\"\n",
    "    elif \"negative\" in generated_text:\n",
    "        print('negative')\n",
    "        return \"Negative\"\n",
    "    else:\n",
    "        return \"Unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative\n",
      "Review: I loved the cinematography and the acting was brilliant.\n",
      "Predicted Sentiment: Negative\n",
      "\n",
      "negative\n",
      "Review: The story was boring and the pacing was way too slow.\n",
      "Predicted Sentiment: Negative\n",
      "\n",
      "positive\n",
      "Review: A complete masterpiece! I would watch it again.\n",
      "Predicted Sentiment: Positive\n",
      "\n",
      "negative\n",
      "Review: Terrible movie, very shallow and i cringed all the time. I regret wasting my time on it.\n",
      "Predicted Sentiment: Negative\n",
      "\n",
      "negative\n",
      "Review: The movie was fantastic, i liked a lot of stuff from it.\n",
      "Predicted Sentiment: Negative\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_reviews = [\n",
    "    \"I loved the cinematography and the acting was brilliant.\",\n",
    "    \"The story was boring and the pacing was way too slow.\",\n",
    "    \"A complete masterpiece! I would watch it again.\",\n",
    "    \"Terrible movie, very shallow and i cringed all the time. I regret wasting my time on it.\",\n",
    "    'The movie was fantastic, i liked a lot of stuff from it.'\n",
    "]\n",
    "\n",
    "# Test each review\n",
    "for review in test_reviews:\n",
    "    sentiment = get_gpt2_prediction(review)\n",
    "    print(f\"Review: {review}\")\n",
    "    print(f\"Predicted Sentiment: {sentiment}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2 generate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load IMDb dataset\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "dataset = load_dataset(\"imdb\")\n",
    "test_df = pd.DataFrame(dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1732"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_df[test_df['text'].str.len() < 400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df[test_df['text'].str.len() < 400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only 50 of each\n",
    "sample_label_0 = test_df[test_df['label'] == 0].sample(n=573, random_state=42)\n",
    "sample_label_1 = test_df[test_df['label'] == 1].sample(n=483, random_state=42)\n",
    "test_df = pd.concat([sample_label_0, sample_label_1])\n",
    "test_df = test_df.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "negative\n",
      "positive\n",
      "positive\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "negative\n",
      "positive\n",
      "positive\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "positive\n",
      "positive\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "positive\n",
      "positive\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "positive\n",
      "positive\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "positive\n",
      "positive\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "negative\n",
      "positive\n",
      "positive\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "positive\n",
      "positive\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "negative\n",
      "positive\n",
      "positive\n",
      "negative\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "negative\n",
      "positive\n",
      "positive\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "positive\n",
      "positive\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "negative\n",
      "positive\n",
      "positive\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "positive\n",
      "positive\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "negative\n",
      "positive\n",
      "positive\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "positive\n",
      "positive\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "positive\n",
      "positive\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "positive\n",
      "positive\n",
      "negative\n",
      "positive\n",
      "positive\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "negative\n",
      "positive\n",
      "positive\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "positive\n",
      "positive\n",
      "negative\n",
      "positive\n",
      "positive\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "negative\n",
      "positive\n",
      "positive\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "positive\n",
      "positive\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "positive\n",
      "positive\n",
      "negative\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "positive\n",
      "positive\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "negative\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "positive\n",
      "positive\n",
      "negative\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "positive\n",
      "positive\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n"
     ]
    }
   ],
   "source": [
    "test_df['GPT2_predictions'] = test_df['text'].apply(lambda x: get_gpt2_prediction(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_parquet('test_df.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.3 compare results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wheus\\AppData\\Roaming\\Python\\Python38\\site-packages\\huggingface_hub\\file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "fine_tuned_model_name = 'C:\\\\Users\\\\wheus\\\\Downloads\\\\fine_tuned_bert_imdb\\\\fine_tuned_bert_imdb'  # Replace with your fine-tuned model path\n",
    "fine_tuned_model = BertForSequenceClassification.from_pretrained(fine_tuned_model_name, output_attentions=True)\n",
    "\n",
    "def get_finetuned_predictions(sentence):\n",
    "    # Example input sentence\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "\n",
    "    # Forward pass through the model to get outputs\n",
    "    outputs = fine_tuned_model(**inputs)\n",
    "    # Get the logits\n",
    "    logits = outputs.logits\n",
    "\n",
    "    # Apply softmax to get probabilities\n",
    "    probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "\n",
    "    # Get the predicted class (0 or 1)\n",
    "    pred_label = torch.argmax(probs, dim=1).item()\n",
    "    return pred_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['finetuned_predictions'] = test_df['text'].apply(lambda x: get_finetuned_predictions(x))\n",
    "test_df.to_parquet('test_df.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wheus\\AppData\\Roaming\\Python\\Python38\\site-packages\\huggingface_hub\\file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "pretrained_model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "def get_pretrained_predictions(sentence):\n",
    "    # Example input sentence\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "\n",
    "    # Forward pass through the model to get outputs\n",
    "    outputs = pretrained_model(**inputs)\n",
    "    # Get the logits\n",
    "    logits = outputs.logits\n",
    "\n",
    "    # Apply softmax to get probabilities\n",
    "    probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "\n",
    "    # Get the predicted class (0 or 1)\n",
    "    pred_label = torch.argmax(probs, dim=1).item()\n",
    "    return pred_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['pretrained_predictions'] = test_df['text'].apply(lambda x: get_pretrained_predictions(x))\n",
    "test_df.to_parquet('test_df.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['GPT2_predictions'] = test_df['GPT2_predictions'].replace({'Positive': 1, 'Negative': 0, 'Unknown': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics for GPT-2 Few-Shot:\n",
      "Accuracy: 0.5786\n",
      "Precision: 0.6105\n",
      "Recall: 0.2174\n",
      "F1-Score: 0.3206\n",
      "\n",
      "Metrics for Pre-trained BERT:\n",
      "Accuracy: 0.4593\n",
      "Precision: 0.4581\n",
      "Recall: 0.9959\n",
      "F1-Score: 0.6275\n",
      "\n",
      "Metrics for Fine-tuned BERT:\n",
      "Accuracy: 0.9366\n",
      "Precision: 0.9143\n",
      "Recall: 0.9503\n",
      "F1-Score: 0.9320\n",
      "\n",
      "Comparative Results:\n",
      "              Model  Accuracy  Precision    Recall  F1-Score\n",
      "0    GPT-2 Few-Shot  0.578598   0.610465  0.217391  0.320611\n",
      "1  Pre-trained BERT  0.459280   0.458095  0.995859  0.627528\n",
      "2   Fine-tuned BERT  0.936553   0.914343  0.950311  0.931980\n",
      "\\begin{tabular}{lrrrr}\n",
      "\\toprule\n",
      "           Model &  Accuracy &  Precision &   Recall &  F1-Score \\\\\n",
      "\\midrule\n",
      "  GPT-2 Few-Shot &  0.578598 &   0.610465 & 0.217391 &  0.320611 \\\\\n",
      "Pre-trained BERT &  0.459280 &   0.458095 & 0.995859 &  0.627528 \\\\\n",
      " Fine-tuned BERT &  0.936553 &   0.914343 & 0.950311 &  0.931980 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wheus\\AppData\\Local\\Temp\\ipykernel_6256\\3785862413.py:44: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  print(results_df.to_latex(index=False))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "# Ground truth labels\n",
    "true_labels = test_df['label']\n",
    "\n",
    "# Predictions from each model\n",
    "gpt2_preds = test_df['GPT2_predictions']\n",
    "pretrained_preds = test_df['pretrained_predictions']\n",
    "finetuned_preds = test_df['finetuned_predictions']\n",
    "\n",
    "# Function to calculate and print metrics\n",
    "def calculate_metrics(true_labels, predictions, model_name):\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    precision = precision_score(true_labels, predictions, average='binary')\n",
    "    recall = recall_score(true_labels, predictions, average='binary')\n",
    "    f1 = f1_score(true_labels, predictions, average='binary')\n",
    "    \n",
    "    print(f\"\\nMetrics for {model_name}:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-Score: {f1:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        \"Model\": model_name,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1-Score\": f1\n",
    "    }\n",
    "\n",
    "# Calculate metrics for each model\n",
    "gpt2_metrics = calculate_metrics(true_labels, gpt2_preds, \"GPT-2 Few-Shot\")\n",
    "pretrained_metrics = calculate_metrics(true_labels, pretrained_preds, \"Pre-trained BERT\")\n",
    "finetuned_metrics = calculate_metrics(true_labels, finetuned_preds, \"Fine-tuned BERT\")\n",
    "\n",
    "# Combine metrics into a DataFrame for comparison\n",
    "import pandas as pd\n",
    "\n",
    "results_df = pd.DataFrame([gpt2_metrics, pretrained_metrics, finetuned_metrics])\n",
    "print(\"\\nComparative Results:\")\n",
    "print(results_df)\n",
    "print(results_df.to_latex(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_parquet('results_df.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.5 attention map for GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2', output_attentions=True)\n",
    "model.eval()\n",
    "\n",
    "def plot_attention_map(model, tokenizer, input_sentence, plot_name):\n",
    "    inputs = tokenizer(input_sentence, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    attentions = outputs.attentions \n",
    "    last_layer_attention = attentions[-1].squeeze(0).detach().numpy()\n",
    "\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'].squeeze().tolist())\n",
    "\n",
    "    def plot_attention_one_head(attention, tokens, title):\n",
    "        clean_tokens = [token.replace(\"Ġ\", \"\") for token in tokens]\n",
    "        fig, ax = plt.subplots(figsize=(10, 10))\n",
    "        im = ax.imshow(attention, cmap=\"viridis\")\n",
    "        ax.set_xticks(range(len(clean_tokens)))\n",
    "        ax.set_yticks(range(len(clean_tokens)))\n",
    "        ax.set_xticklabels(clean_tokens, rotation=90)\n",
    "        ax.set_yticklabels(clean_tokens)\n",
    "        plt.colorbar(im, ax=ax)\n",
    "        plt.title(title)\n",
    "        plt.show()\n",
    "\n",
    "    plot_attention_one_head(last_layer_attention[0], tokens, plot_name)\n",
    "\n",
    "input_sentence = \"Great film, would really recommend!\"\n",
    "plot_name = \"Attention map: Pretrained GPT-2 model. \\nSentence: Great film, would really recommend!\"\n",
    "\n",
    "\n",
    "plot_attention_map(model, tokenizer, input_sentence, plot_name)\n",
    "\n",
    "input_sentence = \"I hated every minute of it\"\n",
    "plot_name = \"Attention map: Pretrained GPT-2 model. \\nSentence: I hated every minute of it\"\n",
    "\n",
    "plot_attention_map(model, tokenizer, input_sentence, plot_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get some information from the train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load IMDb dataset\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "dataset = load_dataset(\"imdb\")\n",
    "train_df = pd.DataFrame(dataset['train'])\n",
    "sample_label_0 = train_df[train_df['label'] == 0].sample(n=2, random_state=42)\n",
    "sample_label_1 = train_df[train_df['label'] == 1].sample(n=2, random_state=42)\n",
    "train_df = pd.concat([sample_label_0, sample_label_1])\n",
    "#train_df = train_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "for index, row in train_df.iterrows():\n",
    "    text = row['text']\n",
    "    print(f'Review: {text}')\n",
    "    sentiment = 'Positive' if row['label'] == 1 else 'Negative'\n",
    "    print(f'Predicted Sentiment: {sentiment}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
